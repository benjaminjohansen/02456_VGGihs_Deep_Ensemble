{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGGish Deep implementation, with Ensemble, Lite\n",
    "The following script will run a lite version (with only a subset of the data) of a VGGish Ensemble method to be used on the [TUT Acoustic Scene data set](http://www.cs.tut.fi/sgn/arg/dcase2017/challenge/task-acoustic-scene-classification#audio-dataset). \n",
    "\n",
    "The first parts of the script will import the data, split as specified at the competion. For this version, a subset of preprossed data has been added - only a subset since there was not space enough for the entire set. To run the full dataset do as described in README.md.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import vggish_input \n",
    "import vggish_params as params\n",
    "import vggish_slim\n",
    "import pickle\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "training = True\n",
    "_NUM_CLASSES = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding unique recording names to ensure that the same recording is only in train or validation set:\n",
    "with open('soundname1.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "    name_1,label_1 = pickle.load(f) # name_1 contains the names of the recordings in subset 1, label_1 contains the corresponding onehot encoded labels\n",
    
    "# The following outcommented lines should be used if the entire dataset is used. However, due to limits in the size of files that can be uploaded to github, the entire dataset is not uploaded.\n",
    "# with open('soundname2.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     name_2,label_2 = pickle.load(f)\n",
    "# with open('soundname3.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     name_3,label_3 = pickle.load(f)\n",
    "# with open('soundname4.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     name_4,label_4 = pickle.load(f) \n",
    "# with open('soundname5.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     name_5,label_5 = pickle.load(f)\n",
    "# with open('soundname6.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     name_6,label_6 = pickle.load(f)\n",
    "# with open('soundname7.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     name_7,label_7 = pickle.load(f)\n",
    "# with open('soundname8.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     name_8,label_8 = pickle.load(f)\n",
    "# with open('soundname9.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     name_9,label_9 = pickle.load(f)\n",
    "# with open('soundname10.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     name_10,label_10 = pickle.load(f)\n",
    "\n",
    "# my_names = np.concatenate((name_1,name_2,name_3,name_4,name_5,name_6,name_7,name_8,name_9,name_10))\n",
    "# my_labels = np.concatenate((label_1,label_2,label_3,label_4,label_5,label_6,label_7,label_8,label_9,label_10))\n",
    "my_names = name_1\n",
    "my_labels = label_1\n",
    "    \n",
    "uni_names, uni_index = np.unique(my_names, return_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding unique labels\n",
    "uni_labels = []\n",
    "for w in range(0,int(len(uni_index))):\n",
    "    c =uni_index[w]\n",
    "    uni_labels.append(my_labels[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting unique recordings in two sets, training and validation\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "sss.get_n_splits(uni_names, uni_labels)\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "for train_index, test_index in sss.split(uni_names, uni_labels):\n",
    "    train.append(train_index) # Recordings for training set\n",
    "    test.append(test_index) # Recordings for validation set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding index of the files from the recordings for training set\n",
    "train_idx = train[0]\n",
    "train_index = [i for i,x in enumerate(my_names) if x == uni_names[train_idx[0]]]\n",
    "for f in range(1,len(train_idx)):\n",
    "    indices = [i for i,x in enumerate(my_names) if x == uni_names[train_idx[f]]]\n",
    "    train_index = np.concatenate((train_index,indices))\n",
    "    \n",
    "\n",
    "\n",
    "# Finding index of the files from the recordings for validation set\n",
    "test_idx = test[0]\n",
    "test_index = [i for i,x in enumerate(my_names) if x == uni_names[test_idx[0]]]\n",
    "for f in range(1,len(test_idx)):\n",
    "    indices = [i for i,x in enumerate(my_names) if x == uni_names[test_idx[f]]]\n",
    "    test_index = np.concatenate((test_index,indices))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mono log mel spectrograms and labels for the development set \n",
    "\n",
    "# Download here : wget https://www.dropbox.com/s/x7t82y3kzmzhdc2/logmel_subset1.pickle?dl=0\n",
    "with open('logmel_subset1.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "    my_features_sub1,my_labels_sub1 = pickle.load(f) \n",
    "    \n",
    "# The following outcommented lines should be used if the entire dataset is used. However, due to limits in the size of files that can be uploaded to github, the entire dataset is not uploaded.\n",
    "# with open('logmel_subset2.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub2,my_labels_sub2 = pickle.load(f)\n",
    "# with open('logmel_subset3.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub3,my_labels_sub3 = pickle.load(f)\n",
    "# with open('logmel_subset4.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub4,my_labels_sub4 = pickle.load(f)\n",
    "# with open('logmel_subset5.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub5,my_labels_sub5 = pickle.load(f)\n",
    "# with open('logmel_subset6.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub6,my_labels_sub6 = pickle.load(f)\n",
    "# with open('logmel_subset7.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub7,my_labels_sub7 = pickle.load(f)\n",
    "# with open('logmel_subset8.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub8,my_labels_sub8 = pickle.load(f)\n",
    "# with open('logmel_subset9.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub9,my_labels_sub9 = pickle.load(f)\n",
    "# with open('logmel_subset10.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub10,my_labels_sub10 = pickle.load(f)\n",
    "\n",
    "# my_features_sub = np.concatenate((my_features_sub1,my_features_sub2,my_features_sub3,my_features_sub4,my_features_sub5,my_features_sub6,my_features_sub7,my_features_sub8,my_features_sub9,my_features_sub10))\n",
    "# my_labels_sub = np.concatenate((my_labels_sub1,my_labels_sub2,my_labels_sub3,my_labels_sub4,my_labels_sub5,my_labels_sub6,my_labels_sub7,my_labels_sub8,my_labels_sub9,my_labels_sub10))\n",
    "my_features_sub = my_features_sub1\n",
    "my_labels_sub = my_labels_sub1\n",
    "\n",
    "my_labels_s = []\n",
    "for w in range(0,int(len(my_labels_sub)/10)):\n",
    "    my_labels_s.append(my_labels_sub[w*10])\n",
    "\n",
    "# Load the right log mel spectrograms and labels for the development set \n",
    "with open('../logmel_subset1_right.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "    my_features_sub1,my_labels_sub1 = pickle.load(f)\n",
    "# Download here : wget https://www.dropbox.com/s/1gd6gmyjrzo6auj/logmel_subset1_right.pickle?dl=0\n",
    "\n",
    "# The following outcommented lines should be used if the entire dataset is used. However, due to limits in the size of files that can be uploaded to github, the entire dataset is not uploaded.\n",
    "# with open('../logmel_subset2_right.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub2,my_labels_sub2 = pickle.load(f)\n",
    "# with open('../logmel_subset3_right.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub3,my_labels_sub3 = pickle.load(f)\n",
    "# with open('../logmel_subset4_right.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub4,my_labels_sub4 = pickle.load(f)\n",
    "# with open('../logmel_subset5_right.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub5,my_labels_sub5 = pickle.load(f)\n",
    "# with open('../logmel_subset6_right.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub6,my_labels_sub6 = pickle.load(f)\n",
    "# with open('../logmel_subset7_right.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub7,my_labels_sub7 = pickle.load(f)\n",
    "# with open('../logmel_subset8_right.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub8,my_labels_sub8 = pickle.load(f)\n",
    "# with open('../logmel_subset9_right.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub9,my_labels_sub9 = pickle.load(f)\n",
    "# with open('../logmel_subset10_right.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub10,my_labels_sub10 = pickle.load(f)\n",
    "\n",
    "# my_features_sub_MD = np.concatenate((my_features_sub1,my_features_sub2,my_features_sub3,my_features_sub4,my_features_sub5,my_features_sub6,my_features_sub7,my_features_sub8,my_features_sub9,my_features_sub10))\n",
    "# my_labels_sub_MD = np.concatenate((my_labels_sub1,my_labels_sub2,my_labels_sub3,my_labels_sub4,my_labels_sub5,my_labels_sub6,my_labels_sub7,my_labels_sub8,my_labels_sub9,my_labels_sub10))\n",
    "my_features_sub_MD = my_features_sub1\n",
    "my_labels_sub_MD = my_labels_sub1\n",
    "\n",
    "# Load the right log-mel spectrograms and labels for the evaluation/test set \n",
    "with open('../Eval1_RIGHT.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "    my_features_sub1,my_labels_sub1 = pickle.load(f)\n",
    "# Download here : wget https://www.dropbox.com/s/9lnnuabmy6jqi4d/Eval1_RIGHT.pickle?dl=0\n",
    "\n",
    "# The following outcommented lines should be used if the entire dataset is used. However, due to limits in the size of files that can be uploaded to github, the entire dataset is not uploaded.\n",
    "# with open('../Eval2_RIGHT.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub2,my_labels_sub2 = pickle.load(f)\n",
    "# with open('../Eval3_RIGHT.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub3,my_labels_sub3 = pickle.load(f)\n",
    "# with open('../Eval4_RIGHT.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub4,my_labels_sub4 = pickle.load(f)\n",
    "\n",
    "# my_features_eval_MD = np.concatenate((my_features_sub1,my_features_sub2,my_features_sub3,my_features_sub4))\n",
    "# my_labels_eval_MD = np.concatenate((my_labels_sub1,my_labels_sub2,my_labels_sub3,my_labels_sub4))\n",
    "my_features_eval_MD = my_features_sub1\n",
    "my_labels_eval_MD = my_labels_sub1\n",
    "\n",
    "\n",
    "my_features_test_MD = []\n",
    "\n",
    "for n in range(0,int(len(my_features_eval_MD)/10)):\n",
    "    my_features_test_MD.append(my_features_eval_MD[n*10:n*10+10])    \n",
    "\n",
    "my_features_test_MD = list(np.reshape(my_features_test_MD,[len(my_features_test_MD)*10,96,64]))\n",
    "\n",
    "# Load the mono log-mel spectrograms and labels for the evaluation/test set \n",
    "with open('logmel_eval1.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "    my_features_sub1,my_labels_sub1 = pickle.load(f)\n",
    "# Download here : wget https://www.dropbox.com/s/6g2ujcajfrrsxja/Eval1.pickle?dl=0\n",
    "    \n",
    "# The following outcommented lines should be used if the entire dataset is used. However, due to limits in the size of files that can be uploaded to github, the entire dataset is not uploaded.\n",
    "# with open('logmel_eval2.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub2,my_labels_sub2 = pickle.load(f)\n",
    "# with open('logmel_eval3.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub3,my_labels_sub3 = pickle.load(f)\n",
    "# with open('logmel_eval4.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     my_features_sub4,my_labels_sub4 = pickle.load(f)\n",
    "\n",
    "# my_features_eval = np.concatenate((my_features_sub1,my_features_sub2,my_features_sub3,my_features_sub4))\n",
    "# my_labels_eval = np.concatenate((my_labels_sub1,my_labels_sub2,my_labels_sub3,my_labels_sub4))\n",
    "my_features_eval = my_features_sub1\n",
    "my_labels_eval = my_labels_sub1\n",
    "\n",
    "my_labels_s_eval = []\n",
    "for w in range(0,int(len(my_labels_eval)/10)):\n",
    "    my_labels_s_eval.append(my_labels_eval[w*10])\n",
    "\n",
    "my_features_test = []\n",
    "my_labels_test = []\n",
    "for n in range(0,int(len(my_features_eval)/10)):\n",
    "    my_features_test.append(my_features_eval[n*10:n*10+10])    \n",
    "    my_labels_test.append(my_labels_eval[n*10:n*10+10])\n",
    "\n",
    "my_features_test = list(np.reshape(my_features_test,[len(my_features_test)*10,96,64]))\n",
    "my_labels_test = list(np.reshape(my_labels_test,[len(my_labels_test)*10,15]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Final model in training mode, if you wish to only run the test, please proceed to next cell'''\n",
    "\n",
    "# Used for saving weights\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "summaries_path = \"tensorboard/%s\" % (timestamp)\n",
    "\n",
    "def main(_):\n",
    "    \n",
    "    # Defining empty array to save results\n",
    "    loss_mean_tr = []\n",
    "    acc_mean_tr = []\n",
    "    acc_major_tr = []\n",
    "\n",
    "    lss_mean_val = []\n",
    "    acc_mean_val = []\n",
    "    acc_major_val = []\n",
    "    \n",
    "    lss_mean_test = []\n",
    "    acc_mean_test = []\n",
    "    acc_major_test = []\n",
    "    \n",
    "    accuracy_mean_test = []\n",
    "    accuracy_major_test = []\n",
    "    loss_mean_test = []\n",
    "    \n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        is_training = tf.placeholder(dtype=tf.bool, shape=None) # Define boelean for using batch norm\n",
    "    \n",
    "        # Define VGGish.\n",
    "        \n",
    "        # VGGish for mono logmel spectrograms \n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                      weights_initializer=tf.truncated_normal_initializer(\n",
    "                           stddev=params.INIT_STDDEV),\n",
    "                       biases_initializer=tf.zeros_initializer(),\n",
    "                       activation_fn=tf.nn.relu,\n",
    "                       trainable=training), \\\n",
    "                 slim.arg_scope([slim.conv2d],\n",
    "                       kernel_size=[3, 3], stride=1, padding='SAME'), \\\n",
    "                 slim.arg_scope([slim.max_pool2d],\n",
    "                       kernel_size=[2, 2], stride=2, padding='SAME'), \\\n",
    "            tf.variable_scope('vggish'):\n",
    "             # Input: a batch of 2-D log-mel-spectrogram patches.\n",
    "            features_ph = tf.placeholder(\n",
    "               tf.float32, shape=(None, params.NUM_FRAMES, params.NUM_BANDS),\n",
    "               name='input_features0')\n",
    "   \n",
    "            # Reshape to 4-D so that we can convolve a batch with conv2d().\n",
    "            net01 = tf.reshape(features_ph, [-1, params.NUM_FRAMES, params.NUM_BANDS, 1])\n",
    "            print(tf.shape(net01))\n",
    "  \n",
    "            # The VGG stack of alternating convolutions and max-pools.\n",
    "            net1 = slim.conv2d(net01, 64, scope='conv1')\n",
    "            net1 = slim.max_pool2d(net1, scope='pool1')\n",
    "            net1 = slim.conv2d(net1, 128, scope='conv2')\n",
    "            net1 = slim.max_pool2d(net1, scope='pool2')\n",
    "            net1 = slim.repeat(net1, 2, slim.conv2d, 256, scope='conv3')\n",
    "            net1 = slim.max_pool2d(net1, scope='pool3')\n",
    "            net1 = slim.repeat(net1, 2, slim.conv2d, 512, scope='conv4')\n",
    "            net1 = slim.max_pool2d(net1, scope='pool4')\n",
    "            vggOut1 = net1\n",
    "  \n",
    "            # Flatten before entering fully-connected layers\n",
    "            net1 = slim.flatten(net1)\n",
    "            netFlat1 = net1\n",
    "            net = slim.repeat(net1, 2, slim.fully_connected, 4096, scope='fc1')\n",
    "        \n",
    "            # The embedding layer.\n",
    "            embeddings1 = slim.fully_connected(net, params.EMBEDDING_SIZE, scope='fc2')\n",
    "            embeddings12 = slim.fully_connected(net1, params.EMBEDDING_SIZE, scope='fc2_1')\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # VGGish for right logmel spectrograms      \n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                       weights_initializer=tf.truncated_normal_initializer(\n",
    "                           stddev=params.INIT_STDDEV),\n",
    "                       biases_initializer=tf.zeros_initializer(),\n",
    "                       activation_fn=tf.nn.relu,\n",
    "                       trainable=training), \\\n",
    "                 slim.arg_scope([slim.conv2d],\n",
    "                       kernel_size=[3, 3], stride=1, padding='SAME'), \\\n",
    "                 slim.arg_scope([slim.max_pool2d],\n",
    "                       kernel_size=[2, 2], stride=2, padding='SAME'), \\\n",
    "            tf.variable_scope('vggish'):\n",
    "            # Input: a batch of 2-D log-mel-spectrogram patches.\n",
    "            features2_ph = tf.placeholder(\n",
    "                tf.float32, shape=(None, params.NUM_FRAMES, params.NUM_BANDS),\n",
    "                name='input_features2')\n",
    "    \n",
    "            # Reshape to 4-D so that we can convolve a batch with conv2d().\n",
    "            net0 = tf.reshape(features2_ph, [-1, params.NUM_FRAMES, params.NUM_BANDS, 1])\n",
    "            print(tf.shape(net0))\n",
    "\n",
    "            # The VGG stack of alternating convolutions and max-pools.\n",
    "            net = slim.conv2d(net0, 64, scope='conv1_MD')\n",
    "            net = slim.max_pool2d(net, scope='pool1_MD')\n",
    "            net = slim.conv2d(net, 128, scope='conv2_MD')\n",
    "            net = slim.max_pool2d(net, scope='pool2_MD')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 256, scope='conv3_MD')\n",
    "            net = slim.max_pool2d(net, scope='pool3_MD')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 512, scope='conv4_MD')\n",
    "            net = slim.max_pool2d(net, scope='pool4_MD')\n",
    "            vggOut2 = net\n",
    "\n",
    "            # Flatten before entering fully-connected layers\n",
    "            net1 = slim.flatten(net)\n",
    "            #netFlat = net1\n",
    "            net = slim.repeat(net1, 2, slim.fully_connected, 4096, scope='fc1_MD')\n",
    "            # The embedding layer.\n",
    "            embeddings2 = slim.fully_connected(net, params.EMBEDDING_SIZE, scope='fc2_MD')\n",
    "            embeddings22 = slim.fully_connected(net1, params.EMBEDDING_SIZE, scope='fc2_MD_1')\n",
    "\n",
    "\n",
    "        # Concatenate the two embedding \n",
    "        embeddings = tf.concat(values=[embeddings12, embeddings22], axis=1, name=\"embeddings\")\n",
    "        vggEmbeddings = tf.concat(values= [vggOut1, vggOut2], axis=1, name = 'vggEmbeddings')\n",
    "        #print(tf.shape(embeddings12), tf.shape(embeddings22))\n",
    "\n",
    "        with tf.variable_scope('mymodel'):\n",
    "            '''Baseline model'''\n",
    "            # The following outcommented lines should be used to run the baseline model\n",
    "#             Add a fully connected layer with 100 units.\n",
    "#             num_units = 100\n",
    "#             fc = slim.fully_connected(embeddings12, num_units)\n",
    "           \n",
    "\n",
    "#            # Add a classifier layer at the end, consisting of parallel logistic\n",
    "#            # classifiers, one per class. This allows for multi-class tasks.\n",
    "#             logits = slim.fully_connected(\n",
    "#              fc, _NUM_CLASSES, activation_fn=None, scope='logits') \n",
    "#             preds = tf.sigmoid(logits, name='prediction')\n",
    "            \n",
    "            \n",
    "\n",
    "            '''Final model'''\n",
    "            l2 = tf.contrib.layers.l2_regularizer(0.1, scope=None) # Adding weight regularizer\n",
    "            num_units1 = 192\n",
    "            fc = tf.layers.dense(embeddings, num_units1, activation=tf.nn.relu, kernel_regularizer=l2) # Dense layer, with ReLU and L2 reguraliser\n",
    "            h_fc1_drop = tf.layers.dropout(fc, rate=0.2, training = is_training) # Adding dropout\n",
    "#             h_fc1_drop = tf.layers.batch_normalization(fc, training=is_training) # Batch norm\n",
    "            \n",
    "            num_units2 = 100\n",
    "            fc2 = tf.layers.dense(h_fc1_drop, num_units2, activation=tf.nn.relu, kernel_regularizer=l2) # Dense layer, with ReLU and L2 reguraliser\n",
    "            h_fc2_drop = tf.layers.dropout(fc2, rate=0.5, training = is_training) # Adding dropout\n",
    "#             h_fc2_drop = tf.layers.batch_normalization(fc2, training=is_training) # Batch norm\n",
    "            \n",
    "            num_units3 = 35\n",
    "            fc3 = tf.layers.dense(h_fc2_drop, num_units3, activation=tf.nn.relu, kernel_regularizer=l2) # Dense layer, with ReLU and L2 reguraliser\n",
    "            h_fc3_drop = tf.layers.dropout(fc3, rate=0.5, training = is_training) # Adding dropout\n",
    "#             h_fc3_drop = tf.layers.batch_normalization(fc3, training=is_training) # Batch norm\n",
    "\n",
    "            # Add a classifier layer at the end, consisting of parallel logistic\n",
    "            # classifiers, one per class. This allows for multi-class tasks.\n",
    "            logits = slim.fully_connected(\n",
    "              h_fc3_drop, _NUM_CLASSES, activation_fn=None, scope='logits')\n",
    "\n",
    "            preds = tf.nn.softmax(logits, name='prediction')\n",
    "\n",
    "            # Add training ops.\n",
    "            with tf.variable_scope('train'):\n",
    "                global_step = tf.Variable(\n",
    "                    0, name='global_step', trainable=False,\n",
    "                    collections=[tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                                 tf.GraphKeys.GLOBAL_STEP])\n",
    "\n",
    "                # Labels are assumed to be fed as a batch multi-hot vectors, with\n",
    "                # a 1 in the position of each positive class label, and 0 elsewhere.\n",
    "                labels = tf.placeholder(\n",
    "                    tf.float32, shape=(None, _NUM_CLASSES), name='labels')\n",
    "\n",
    "                # Cross-entropy label loss.\n",
    "                xent = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=logits, labels=labels, name='xent')\n",
    "                loss = tf.reduce_mean(xent, name='loss_op')\n",
    "                tf.summary.scalar('loss', loss)\n",
    "\n",
    "                # Accuracy\n",
    "                argmax_y = tf.to_int32(tf.argmax(preds, axis=1))\n",
    "                argmax_t = tf.to_int32(tf.argmax(labels, axis=1))\n",
    "                correct = tf.to_float(tf.equal(argmax_y,argmax_t))\n",
    "                accuracy1 = tf.reduce_mean(correct, name='accuracy_op')\n",
    "                tf.summary.scalar('accuracy1', accuracy1)\n",
    "                \n",
    "                '''Uncomment for decaying learning rate'''\n",
    "#                 starter_learning_rate = 0.1\n",
    "#                 learningRate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            100000, 0.96, staircase=True)\n",
    "                \n",
    "                learningRate = params.LEARNING_RATE # 1e-4, change to set custom learning rate\n",
    "                \n",
    "                # We use the same optimizer and hyperparameters as used to train VGGish.\n",
    "                optimizer = tf.train.AdamOptimizer(\n",
    "                    learning_rate=learningRate, # 1e-4\n",
    "                    epsilon=params.ADAM_EPSILON) # 1e-8\n",
    "                train_op = optimizer.minimize(loss, global_step=global_step, name='train_op')\n",
    "                \n",
    "                saver = tf.train.Saver()\n",
    "\n",
    "        # Initialize all variables in the model, and then load the pre-trained\n",
    "        # VGGish checkpoint.\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        vggish_slim.load_vggish_slim_checkpoint(sess, './vggish_model.ckpt') # Load VGGish weights\n",
    "\n",
    "        # Define a writer for the tensorboard    \n",
    "        writer = tf.summary.FileWriter('log/train/'+summaries_path, sess.graph)\n",
    "        writer_val = tf.summary.FileWriter('log/val/'+summaries_path, sess.graph)\n",
    "\n",
    "        # Locate all the tensors and ops we need for the training loop.\n",
    "        labels_tensor = sess.graph.get_tensor_by_name('mymodel/train/labels:0')\n",
    "        global_step_tensor = sess.graph.get_tensor_by_name(\n",
    "            'mymodel/train/global_step:0')\n",
    "        loss_tensor = sess.graph.get_tensor_by_name('mymodel/train/loss_op:0')\n",
    "        prediction_op = sess.graph.get_operation_by_name('mymodel/prediction')\n",
    "\n",
    "        # Training loop over epochs and batches\n",
    "        num_epochs = 15\n",
    "        batch_size_train = 16\n",
    "        batch_size_val = 16\n",
    "\n",
    "        # Merging all summaries for the tensorboard\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        start = time.perf_counter() # Measure time start\n",
    "        cur_epoch = 0\n",
    "        \n",
    "        # Start training of the model\n",
    "        for i in range(0,num_epochs):\n",
    "            startEpoch = time.perf_counter() # time per epoch\n",
    "            num_batch_train = math.ceil(len(train_index)/batch_size_train)\n",
    "\n",
    "            # Taking out the spectrograms and labels corresponding to the training set \n",
    "            my_features_train = []\n",
    "            my_features_train_MD = []\n",
    "            idx_train = shuffle(train_index)\n",
    "            \n",
    "            my_labels_train = []\n",
    "            for h in range(0,len(idx_train)):\n",
    "                my_features_train.append(my_features_sub[idx_train[h]*10:idx_train[h]*10+10])\n",
    "                my_features_train_MD.append(my_features_sub_MD[idx_train[h]*10:idx_train[h]*10+10])\n",
    "                my_labels_train.append(my_labels_sub[idx_train[h]*10:idx_train[h]*10+10])\n",
    "\n",
    "            my_features_train = list(np.reshape(my_features_train,[len(my_features_train)*10,96,64]))\n",
    "            my_features_train_MD = list(np.reshape(my_features_train_MD,[len(my_features_train_MD)*10,96,64]))\n",
    "            my_labels_train = list(np.reshape(my_labels_train,[len(my_labels_train)*10,15]))\n",
    "            \n",
    "            # Defining empty arrays of measures to append\n",
    "            accuracy_mean = []\n",
    "            accuracy_major_mean = []\n",
    "            loss_mean = []\n",
    "\n",
    "            my_features_train, my_features_train_MD, my_labels_train = shuffle(my_features_train,my_features_train_MD,my_labels_train) # Shuffle to not use majority vote\n",
    "            \n",
    "            # Run the training set\n",
    "            for j in range(0,num_batch_train):\n",
    "                # Taking out specrograms and labels corresponding to one batch\n",
    "                features = my_features_train[j*batch_size_train*10:(j+1)*batch_size_train*10]\n",
    "                features2 = my_features_train_MD[j*batch_size_train*10:(j+1)*batch_size_train*10]\n",
    "                labels = my_labels_train[j*batch_size_train*10:(j+1)*batch_size_train*10]\n",
    "\n",
    "                # Training on one batch\n",
    "                [num_steps, loss, acc1, _, pred, summ] = sess.run(\n",
    "                    [global_step_tensor, loss_tensor, accuracy1, train_op, preds, merged],\n",
    "                    feed_dict={features_ph: features, features2_ph: features2, labels_tensor: labels, is_training: True})\n",
    "                \n",
    "                # Majority vote - not used for training in the final model\n",
    "                pred10 = []\n",
    "                label10 = []\n",
    "                for l in range(0,pred.shape[0],10):\n",
    "                    pred10.append([l0+l1+l2+l3+l4+l5+l6+l7+l8+l9 for l0,l1,l2,l3,l4,l5,l6,l7,l8,l9 in zip(pred[l+0],pred[l+1],pred[l+2],pred[l+3],pred[l+4],pred[l+5],pred[l+6],pred[l+7],pred[l+8],pred[l+9],)]) \n",
    "                    label10.append(labels[l])\n",
    "                argmax_y = np.argmax(pred10,axis=1)\n",
    "                argmax_t = np.argmax(label10,axis=1)\n",
    "                correct = np.float32(np.equal(argmax_y,argmax_t))\n",
    "                accuracy10 = np.mean(correct)\n",
    "\n",
    "                # Appending measures\n",
    "                accuracy_mean.append(acc1)\n",
    "                accuracy_major_mean.append(accuracy10)\n",
    "                loss_mean.append(loss)\n",
    "\n",
    "                writer.add_summary(summ, num_steps)\n",
    "#                 print('Epoch %d, batch %d: loss %g, accuracy1 %f, acc major vote %f' % (i+1, j+1, loss, acc1, accuracy10))\n",
    "\n",
    "            # Finding mean for all batches in current epoch\n",
    "            loss_mean_tr.append(sum(loss_mean)/len(loss_mean))\n",
    "            acc_mean_tr.append(sum(accuracy_mean)/len(accuracy_mean))\n",
    "            acc_major_tr.append(sum(accuracy_major_mean)/len(accuracy_major_mean))\n",
    "            \n",
    "            print('Mean values train: loss %g, accuracy %f, accuracy major %f' % (sum(loss_mean)/len(loss_mean), sum(accuracy_mean)/len(accuracy_mean), sum(accuracy_major_mean)/len(accuracy_major_mean)))\n",
    "\n",
    "            # Running the validation set\n",
    "            idx_val = shuffle(test_index)\n",
    "            my_features_val = []\n",
    "            my_features_val_MD = []\n",
    "            my_labels_val = []\n",
    "\n",
    "            for k in range(0,len(idx_val)):\n",
    "                # Taking out spectrograms and labels corresponding to validation set\n",
    "                my_features_val.append(my_features_sub[idx_val[k]*10:idx_val[k]*10+10])\n",
    "                my_features_val_MD.append(my_features_sub_MD[idx_val[k]*10:idx_val[k]*10+10])\n",
    "                my_labels_val.append(my_labels_sub[idx_val[k]*10:idx_val[k]*10+10])\n",
    "\n",
    "            my_features_val = list(np.reshape(my_features_val,[len(my_features_val)*10,96,64]))\n",
    "            my_features_val_MD = list(np.reshape(my_features_val_MD,[len(my_features_val_MD)*10,96,64]))\n",
    "            \n",
    "            my_labels_val = list(np.reshape(my_labels_val,[len(my_labels_val)*10,15]))\n",
    "            \n",
    "            num_batch_val = math.ceil(len(test_index)/batch_size_val)\n",
    "\n",
    "            # Empty arrays for measures to append\n",
    "            accuracy_mean_val = []\n",
    "            accuracy_major_val = []\n",
    "            loss_mean_val = []\n",
    "\n",
    "            for j in range(0,num_batch_val):\n",
    "                # Taking out spectrograms and labels corresponding to one batch\n",
    "                features_val = my_features_val[j*batch_size_val*10:(j+1)*batch_size_val*10]\n",
    "                features_val_MD = my_features_val_MD[j*batch_size_val*10:(j+1)*batch_size_val*10]\n",
    "                labels_val = my_labels_val[j*batch_size_val*10:(j+1)*batch_size_val*10]\n",
    "                \n",
    "                # Running validation set without training\n",
    "                [num_steps2, valloss, valacc1,valpred, summ2] = sess.run(\n",
    "                    [global_step_tensor, loss_tensor, accuracy1,preds, merged],\n",
    "                    feed_dict={features_ph:features_val,features2_ph:features_val_MD, labels_tensor: labels_val, is_training: False})\n",
    "                \n",
    "                # Majority vote\n",
    "                pred10_val = []\n",
    "                label10_val = []\n",
    "                for l in range(0,valpred.shape[0],10):\n",
    "                    pred10_val.append([l0+l1+l2+l3+l4+l5+l6+l7+l8+l9 for l0,l1,l2,l3,l4,l5,l6,l7,l8,l9 in zip(valpred[l+0],valpred[l+1],valpred[l+2],valpred[l+3],valpred[l+4],valpred[l+5],valpred[l+6],valpred[l+7],valpred[l+8],valpred[l+9],)]) \n",
    "                    label10_val.append(labels_val[l])\n",
    "                argmax_y_val = np.argmax(pred10_val,axis=1)\n",
    "                argmax_t_val = np.argmax(label10_val,axis=1)\n",
    "                correct_val = np.float32(np.equal(argmax_y_val,argmax_t_val))\n",
    "                accuracy10_val = np.mean(correct_val)\n",
    "                \n",
    "                # Appending measures\n",
    "                accuracy_mean_val.append(valacc1)\n",
    "                accuracy_major_val.append(accuracy10_val)\n",
    "                loss_mean_val.append(valloss)\n",
    "\n",
    "                writer.add_summary(summ2, num_steps2)\n",
    "#                 print('Epoch %d, val_batch %d: loss %g, accuracy1 %f, acc major vote %f' % (i+1, j+1, valloss, valacc1, accuracy10_val))\n",
    "            \n",
    "            # Mean values for all batches in current epoch\n",
    "            lss_mean_val.append(sum(loss_mean_val)/len(loss_mean_val))\n",
    "            acc_mean_val.append(sum(accuracy_mean_val)/len(accuracy_mean_val))\n",
    "            acc_major_val.append(sum(accuracy_major_val)/len(accuracy_major_val))\n",
    "            \n",
    "            print('Mean values val: loss %g, accuracy %f, accuracy major %f' % (sum(loss_mean_val)/len(loss_mean_val), sum(accuracy_mean_val)/len(accuracy_mean_val), sum(accuracy_major_val)/len(accuracy_major_val)))\n",
    "\n",
    "            cur_epoch += 1\n",
    "            elapsedEpoch = (time.perf_counter() - startEpoch)/60\n",
    "            print('Elapsed time for Epoch %d: %.3f minutes.' % (cur_epoch, elapsedEpoch))\n",
    "            \n",
    "        # After training the model, run the test\n",
    "        batch_size_test = batch_size_val # change to custom batch size, as default same as validation\n",
    "        num_batch_test = math.ceil(len(my_features_eval)/(batch_size_test*10))\n",
    "        \n",
    "        for i in range(0,num_batch_test):\n",
    "            # Taking out spectrograms and labels corresponding to one batch\n",
    "            features_test = my_features_test[j*batch_size_test*10:(j+1)*batch_size_test*10]\n",
    "            features_test_MD = my_features_test_MD[j*batch_size_val*10:(j+1)*batch_size_val*10]\n",
    "            labels_test = my_labels_test[j*batch_size_test*10:(j+1)*batch_size_test*10]\n",
    "            \n",
    "            # Running without training\n",
    "            [num_steps3, testloss, testacc1,testpred, summ3] = sess.run(\n",
    "                [global_step_tensor, loss_tensor, accuracy1,preds, merged],\n",
    "                feed_dict={features_ph:features_test, features2_ph:features_test_MD, labels_tensor: labels_test, is_training: False})\n",
    "\n",
    "            # Majority vote\n",
    "            pred10_test = []\n",
    "            label10_test = []\n",
    "            for l in range(0,testpred.shape[0],10):\n",
    "                pred10_test.append([l0+l1+l2+l3+l4+l5+l6+l7+l8+l9 for l0,l1,l2,l3,l4,l5,l6,l7,l8,l9 in zip(testpred[l+0],testpred[l+1],testpred[l+2],testpred[l+3],testpred[l+4],testpred[l+5],testpred[l+6],testpred[l+7],testpred[l+8],testpred[l+9],)]) \n",
    "                label10_test.append(labels_test[l])\n",
    "            argmax_y_test = np.argmax(pred10_test,axis=1)\n",
    "            argmax_t_test = np.argmax(label10_test,axis=1)\n",
    "            correct_test = np.float32(np.equal(argmax_y_test,argmax_t_test))\n",
    "            accuracy10_test = np.mean(correct_test)\n",
    "\n",
    "            # Appending measures\n",
    "            accuracy_mean_test.append(testacc1)\n",
    "            accuracy_major_test.append(accuracy10_test)\n",
    "            loss_mean_test.append(testloss)\n",
    "\n",
    "            writer.add_summary(summ3, num_steps3)\n",
    "#                 print('test_batch %d: loss %g, accuracy1 %f, acc major vote %f' % (j+1, testloss, testacc1, accuracy10_test))\n",
    "\n",
    "            # Average\n",
    "            lss_mean_test.append(sum(loss_mean_test)/len(loss_mean_test))\n",
    "            acc_mean_test.append(sum(accuracy_mean_test)/len(accuracy_mean_test))\n",
    "            acc_major_test.append(sum(accuracy_major_test)/len(accuracy_major_test))\n",
    "        print('\\n','****', '\\n')\n",
    "        print('Mean values test: loss %g, accuracy %f, accuracy major %f' % (sum(loss_mean_test)/len(loss_mean_test), sum(accuracy_mean_test)/len(accuracy_mean_test), sum(accuracy_major_test)/len(accuracy_major_test)))\n",
    "        print('\\n','****', '\\n')\n",
    "        elapsed = (time.perf_counter() - start)/60\n",
    "        print('Elapsed total time %.3f minutes.' % elapsed) \n",
    "        print('Success!')\n",
    "        \n",
    "        # Save all mean value to dataframe, then save to CSV\n",
    "        mean_results = pd.DataFrame({'lossTr': loss_mean_tr,\n",
    "                                     'accTr': acc_mean_tr,\n",
    "                                     'accMajorTr': acc_major_tr,\n",
    "                                     'lossVal': lss_mean_val,\n",
    "                                     'accVal': acc_mean_val,\n",
    "                                     'accMajorVal': acc_major_val\n",
    "                                    })\n",
    "        mean_results.to_csv('VGGish_final.csv', index = False)\n",
    "    \n",
    "#         save_path = saver.save(sess, \"./final.ckpt\") # Uncomment to save model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Run the following script to test a pretrained model, by loading weights.'''\n",
    "\n",
    "def main(_):\n",
    "    \n",
    "    \n",
    "    lss_mean_test = []\n",
    "    acc_mean_test = []\n",
    "    acc_major_test = []\n",
    "    \n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        is_training = tf.placeholder(dtype=tf.bool, shape=None) # Define boelean for using batch norm\n",
    "        # Define VGGish.\n",
    "        \n",
    "        # VGGish for mono logmel spectrograms \n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                      weights_initializer=tf.truncated_normal_initializer(\n",
    "                           stddev=params.INIT_STDDEV),\n",
    "                       biases_initializer=tf.zeros_initializer(),\n",
    "                       activation_fn=tf.nn.relu,\n",
    "                       trainable=training), \\\n",
    "                 slim.arg_scope([slim.conv2d],\n",
    "                       kernel_size=[3, 3], stride=1, padding='SAME'), \\\n",
    "                 slim.arg_scope([slim.max_pool2d],\n",
    "                       kernel_size=[2, 2], stride=2, padding='SAME'), \\\n",
    "            tf.variable_scope('vggish'):\n",
    "             # Input: a batch of 2-D log-mel-spectrogram patches.\n",
    "            features_ph = tf.placeholder(\n",
    "               tf.float32, shape=(None, params.NUM_FRAMES, params.NUM_BANDS),\n",
    "               name='input_features0')\n",
    "   \n",
    "            # Reshape to 4-D so that we can convolve a batch with conv2d().\n",
    "            net01 = tf.reshape(features_ph, [-1, params.NUM_FRAMES, params.NUM_BANDS, 1])\n",
    "            print(tf.shape(net01))\n",
    "  \n",
    "            # The VGG stack of alternating convolutions and max-pools.\n",
    "            net1 = slim.conv2d(net01, 64, scope='conv1')\n",
    "            net1 = slim.max_pool2d(net1, scope='pool1')\n",
    "            net1 = slim.conv2d(net1, 128, scope='conv2')\n",
    "            net1 = slim.max_pool2d(net1, scope='pool2')\n",
    "            net1 = slim.repeat(net1, 2, slim.conv2d, 256, scope='conv3')\n",
    "            net1 = slim.max_pool2d(net1, scope='pool3')\n",
    "            net1 = slim.repeat(net1, 2, slim.conv2d, 512, scope='conv4')\n",
    "            net1 = slim.max_pool2d(net1, scope='pool4')\n",
    "            vggOut1 = net1\n",
    "  \n",
    "            # Flatten before entering fully-connected layers\n",
    "            net1 = slim.flatten(net1)\n",
    "            netFlat1 = net1\n",
    "            net = slim.repeat(net1, 2, slim.fully_connected, 4096, scope='fc1')\n",
    "        \n",
    "            # The embedding layer.\n",
    "            embeddings1 = slim.fully_connected(net, params.EMBEDDING_SIZE, scope='fc2')\n",
    "            embeddings12 = slim.fully_connected(net1, params.EMBEDDING_SIZE, scope='fc2_1')\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # VGGish for right logmel spectrograms      \n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                       weights_initializer=tf.truncated_normal_initializer(\n",
    "                           stddev=params.INIT_STDDEV),\n",
    "                       biases_initializer=tf.zeros_initializer(),\n",
    "                       activation_fn=tf.nn.relu,\n",
    "                       trainable=training), \\\n",
    "                 slim.arg_scope([slim.conv2d],\n",
    "                       kernel_size=[3, 3], stride=1, padding='SAME'), \\\n",
    "                 slim.arg_scope([slim.max_pool2d],\n",
    "                       kernel_size=[2, 2], stride=2, padding='SAME'), \\\n",
    "            tf.variable_scope('vggish'):\n",
    "            # Input: a batch of 2-D log-mel-spectrogram patches.\n",
    "            features2_ph = tf.placeholder(\n",
    "                tf.float32, shape=(None, params.NUM_FRAMES, params.NUM_BANDS),\n",
    "                name='input_features2')\n",
    "    \n",
    "            # Reshape to 4-D so that we can convolve a batch with conv2d().\n",
    "            net0 = tf.reshape(features2_ph, [-1, params.NUM_FRAMES, params.NUM_BANDS, 1])\n",
    "            print(tf.shape(net0))\n",
    "\n",
    "            # The VGG stack of alternating convolutions and max-pools.\n",
    "            net = slim.conv2d(net0, 64, scope='conv1_MD')\n",
    "            net = slim.max_pool2d(net, scope='pool1_MD')\n",
    "            net = slim.conv2d(net, 128, scope='conv2_MD')\n",
    "            net = slim.max_pool2d(net, scope='pool2_MD')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 256, scope='conv3_MD')\n",
    "            net = slim.max_pool2d(net, scope='pool3_MD')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 512, scope='conv4_MD')\n",
    "            net = slim.max_pool2d(net, scope='pool4_MD')\n",
    "            vggOut2 = net\n",
    "\n",
    "            # Flatten before entering fully-connected layers\n",
    "            net1 = slim.flatten(net)\n",
    "            #netFlat = net1\n",
    "            net = slim.repeat(net1, 2, slim.fully_connected, 4096, scope='fc1_MD')\n",
    "            # The embedding layer.\n",
    "            embeddings2 = slim.fully_connected(net, params.EMBEDDING_SIZE, scope='fc2_MD')\n",
    "            embeddings22 = slim.fully_connected(net1, params.EMBEDDING_SIZE, scope='fc2_MD_1')\n",
    "\n",
    "\n",
    "        # Concatenate the two embedding \n",
    "        embeddings = tf.concat(values=[embeddings12, embeddings22], axis=1, name=\"embeddings\")\n",
    "        vggEmbeddings = tf.concat(values= [vggOut1, vggOut2], axis=1, name = 'vggEmbeddings')\n",
    "        #print(tf.shape(embeddings12), tf.shape(embeddings22))\n",
    "\n",
    "        with tf.variable_scope('mymodel'):\n",
    "            '''Baseline model'''\n",
    "              # Use the outcommented lines to use baseline model\n",
    "#             Add a fully connected layer with 100 units.\n",
    "#             num_units = 100\n",
    "#             fc = slim.fully_connected(embeddings12, num_units)\n",
    "           \n",
    "#            # Add a classifier layer at the end, consisting of parallel logistic\n",
    "#            # classifiers, one per class. This allows for multi-class tasks.\n",
    "#             logits = slim.fully_connected(\n",
    "#              fc, _NUM_CLASSES, activation_fn=None, scope='logits') \n",
    "#             preds = tf.sigmoid(logits, name='prediction')\n",
    "            \n",
    "            \n",
    "\n",
    "            '''Final model'''\n",
    "            l2 = tf.contrib.layers.l2_regularizer(0.1, scope=None) # Adding weight regularizer\n",
    "            num_units1 = 192\n",
    "            fc = tf.layers.dense(embeddings, num_units1, activation=tf.nn.relu, kernel_regularizer=l2) # Dense layer, with ReLU and L2 reguraliser\n",
    "            h_fc1_drop = tf.layers.dropout(fc, rate=0.2, training = is_training) # Adding dropout layer\n",
    "#             h_fc1_drop = tf.layers.batch_normalization(fc, training=is_training) # Batch norm\n",
    "            \n",
    "            num_units2 = 100\n",
    "            fc2 = tf.layers.dense(h_fc1_drop, num_units2, activation=tf.nn.relu, kernel_regularizer=l2) # Dense layer, with ReLU and L2 reguraliser\n",
    "            h_fc2_drop = tf.layers.dropout(fc2, rate=0.5, training = is_training) # Adding dropout layer\n",
    "#             h_fc2_drop = tf.layers.batch_normalization(fc2, training=is_training) # Batch norm\n",
    "            \n",
    "            num_units3 = 35\n",
    "            fc3 = tf.layers.dense(h_fc2_drop, num_units3, activation=tf.nn.relu, kernel_regularizer=l2) # Dense layer, with ReLU and L2 reguraliser\n",
    "            h_fc3_drop = tf.layers.dropout(fc3, rate=0.5, training = is_training) # Adding dropout layer\n",
    "#             h_fc3_drop = tf.layers.batch_normalization(fc3, training=is_training) # Batch norm\n",
    "\n",
    "            # Add a classifier layer at the end, consisting of parallel logistic\n",
    "            # classifiers, one per class. This allows for multi-class tasks.\n",
    "            logits = slim.fully_connected(\n",
    "              h_fc3_drop, _NUM_CLASSES, activation_fn=None, scope='logits')\n",
    "\n",
    "            preds = tf.nn.softmax(logits, name='prediction')\n",
    "\n",
    "            # Add training ops.\n",
    "            with tf.variable_scope('train'):\n",
    "                global_step = tf.Variable(\n",
    "                    0, name='global_step', trainable=False,\n",
    "                    collections=[tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                                 tf.GraphKeys.GLOBAL_STEP])\n",
    "\n",
    "                # Labels are assumed to be fed as a batch multi-hot vectors, with\n",
    "                # a 1 in the position of each positive class label, and 0 elsewhere.\n",
    "                labels = tf.placeholder(\n",
    "                    tf.float32, shape=(None, _NUM_CLASSES), name='labels')\n",
    "\n",
    "                # Cross-entropy label loss.\n",
    "                xent = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=logits, labels=labels, name='xent')\n",
    "                loss = tf.reduce_mean(xent, name='loss_op')\n",
    "                tf.summary.scalar('loss', loss)\n",
    "\n",
    "                # Accuracy\n",
    "                argmax_y = tf.to_int32(tf.argmax(preds, axis=1))\n",
    "                argmax_t = tf.to_int32(tf.argmax(labels, axis=1))\n",
    "                correct = tf.to_float(tf.equal(argmax_y,argmax_t))\n",
    "                accuracy1 = tf.reduce_mean(correct, name='accuracy_op')\n",
    "                tf.summary.scalar('accuracy1', accuracy1)\n",
    "\n",
    "                # We use the same optimizer and hyperparameters as used to train VGGish.\n",
    "                optimizer = tf.train.AdamOptimizer(\n",
    "                    learning_rate=params.LEARNING_RATE,\n",
    "                    epsilon=params.ADAM_EPSILON)\n",
    "                train_op = optimizer.minimize(loss, global_step=global_step, name='train_op')\n",
    "                \n",
    "                saver = tf.train.Saver()\n",
    "\n",
    "        # Initialize all variables in the model, and then load the pre-trained\n",
    "        # VGGish checkpoint.\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        saver.restore(sess, './VGGish_concate1.ckpt')\n",
    "\n",
    "        writer = tf.summary.FileWriter('log/train/'+summaries_path, sess.graph)\n",
    "\n",
    "        # All tensors and ops\n",
    "        labels_tensor = sess.graph.get_tensor_by_name('mymodel/train/labels:0')\n",
    "        global_step_tensor = sess.graph.get_tensor_by_name(\n",
    "            'mymodel/train/global_step:0')\n",
    "        loss_tensor = sess.graph.get_tensor_by_name('mymodel/train/loss_op:0')\n",
    "        prediction_op = sess.graph.get_operation_by_name('mymodel/prediction')\n",
    "\n",
    "        \n",
    "        merged = tf.summary.merge_all()\n",
    "        \n",
    "        \n",
    "        # Running test set in batches\n",
    "        batch_size_test = 30\n",
    "        num_batch_test = math.ceil(len(my_features_eval)/(batch_size_test*10))\n",
    "        \n",
    "        # Defining measures to append\n",
    "        accuracy_mean_test = []\n",
    "        accuracy_major_test = []\n",
    "        loss_mean_test = []\n",
    "\n",
    "        for j in range(0,num_batch_test):\n",
    "            # Taking out spectrograms and labels corresponding to one batch\n",
    "            features_test = my_features_test[j*batch_size_test*10:(j+1)*batch_size_test*10]\n",
    "            features_test_MD = my_features_test_MD[j*batch_size_val*10:(j+1)*batch_size_val*10]\n",
    "            labels_test = my_labels_test[j*batch_size_test*10:(j+1)*batch_size_test*10]\n",
    "            \n",
    "            # Running \n",
    "            [num_steps3, testloss, testacc1,testpred, summ3] = sess.run(\n",
    "                [global_step_tensor, loss_tensor, accuracy1,preds, merged],\n",
    "                feed_dict={features_ph:features_test, features2_ph:features_test_MD, labels_tensor: labels_test, is_training: False})\n",
    "            \n",
    "            # Majority vote\n",
    "            pred10_test = []\n",
    "            label10_test = []\n",
    "            for l in range(0,testpred.shape[0],10):\n",
    "                pred10_test.append([l0+l1+l2+l3+l4+l5+l6+l7+l8+l9 for l0,l1,l2,l3,l4,l5,l6,l7,l8,l9 in zip(testpred[l+0],testpred[l+1],testpred[l+2],testpred[l+3],testpred[l+4],testpred[l+5],testpred[l+6],testpred[l+7],testpred[l+8],testpred[l+9],)])\n",
    "                label10_test.append(labels_test[l])\n",
    "            argmax_y_test = np.argmax(pred10_test,axis=1)\n",
    "            argmax_t_test = np.argmax(label10_test,axis=1)\n",
    "            correct_test = np.float32(np.equal(argmax_y_test,argmax_t_test))\n",
    "            accuracy10_test = np.mean(correct_test)\n",
    "            \n",
    "            # Appending measures\n",
    "            accuracy_mean_test.append(testacc1)\n",
    "            accuracy_major_test.append(accuracy10_test)\n",
    "            loss_mean_test.append(testloss)\n",
    "\n",
    "            writer.add_summary(summ3, num_steps3)\n",
    "            print('test_batch %d: loss %g, accuracy1 %f, acc major vote %f' % (j+1, testloss, testacc1, accuracy10_test))\n",
    "        \n",
    "        # Average for all batches\n",
    "        lss_mean_test.append(sum(loss_mean_test)/len(loss_mean_test))\n",
    "        acc_mean_test.append(sum(accuracy_mean_test)/len(accuracy_mean_test))\n",
    "        acc_major_test.append(sum(accuracy_major_test)/len(accuracy_major_test))\n",
    "        \n",
    "        print('Mean values test: loss %g, accuracy %f, accuracy major %f' % (sum(loss_mean_test)/len(loss_mean_test), sum(accuracy_mean_test)/len(accuracy_mean_test), sum(accuracy_major_test)/len(accuracy_major_test)))\n",
    "\n",
    "    print('Succes!')\n",
    "    \n",
    "    mean_results = pd.DataFrame({'lossTest': lss_mean_test,\n",
    "                                 'accTest': acc_mean_test,\n",
    "                                 'accMajorTest': acc_major_test\n",
    "                                })\n",
    "    mean_results.to_csv('results_test.csv', index = False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
